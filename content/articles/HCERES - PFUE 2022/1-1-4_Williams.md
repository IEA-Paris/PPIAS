---
DOI: 10.5072/zenodo.1205033
Zid: 1205033
abstract: |-
  Evaluation of Social Sciences and Humanities in Europe.
   Hcéres Colloquium Proceedings - Paris IAS,  16-17 May 2022
  Session 1 "Books and Monographs" - Books in Evaluation
article_title: ' Not publish and survive: Less may be better in book publication'
authors:
  - firstname: Geoffrey
    lastname: Williams
    picture: ''
    picture_copyright: ''
    positions_and_institutions:
      - institution: Université Bretagne Sud, France
        positions: []
    reference: ''
    social_channels:
      google_scholar: ''
      instagram: ''
      linkedin: ''
      mendeley: ''
      orcid: ''
      researchgate: ''
      twitter: ''
      website: ''
      wikipedia: ''
bibliography: /documents-exportes_114.bib
custom_pdf: ''
date: 2022-05-16T09:41:05.000Z
disciplines: ''
highlight: false
issue: content/issues/HCERES - PFUE 2022.md
keywords: []
language: English
links:
  bucket: https://sandbox.zenodo.org/api/files/296b91cb-2794-4604-b60b-ece0fab3e122
needDOI: true
picture: ''
picture_copyright: ''
published: true
yt: ''

---







From a technocratic view of academia, indicators provide an ideal way of assessing a certain view of ‘performance’ in a quantitative manner. However, quantitative data without a qualitative analysis as to how and why researchers disseminate their work is meaningless. Normally, academic research should not be about performing to be assessed, but about bringing meaningful insights to different audiences. In addition, indicators can have perverse effects when used to evaluate as researchers who will adapt to fit in with requirements that are external to the needs of research purely in order to obtain jobs and promotion. Certain potential dissemination outlets also adapt to a ‘market’, and quantity can quickly lessen quality, the latter being intrinsically difficult to define. This is true for journals where parasite publishers are rife, but also in book publishing. The following two examples illustrate the constant publicity to which researchers are subjected.

_“XXX are inviting proposals for academic books and edited collections in Humanities and Social Sciences. We publish in all major fields of academic research and practice, including Humanities and Social Sciences, Physical Sciences, Life Sciences, and Health Sciences. To submit a book proposal, please visit our website, where you can complete a Book Proposal Form, and find out more about us.”_

_“Publishing in a book series is one of the best ways to expand on your work. Writing a book for our XXX series can offer you more space—more breadth and depth—than a journal article. However, despite being an extremely rewarding experience, it can also be an intimidating task, which is why our experienced and expert Editors are here to help.”_

It is not necessary to name either as both fill an important role in the research dissemination infrastructure. However, the first relies on quantity and a constant flow of direct mailing and whilst not open access, they have no author charges. On the other hand, their quality control is less clear, particularly as concerns ‘collective’ works and proceedings. The second is a serious major publisher looking for custom, and they would seriously vet any proposal. However, in both cases, the researchers and publishers are responding to a market and questions should be raised as to motivations behind publishing. Is publication research or evaluation driven? Is there another way to disseminate research in book form?

Studies often repeat the mantra that the social sciences and humanities publish books, whereas the sciences publish in journals. Like all stereotypes, there is some truth, and books do hold considerable kudos in the humanities, and to a lesser extent in the social sciences. However, the real situation is far more complex and subject to a wide variety of disciplinary and institutional factors.

Studies often repeat the mantra that the social sciences and humanities publish books, whereas the sciences publish in journals. Like all stereotypes, there is some truth, and books do hold considerable kudos in the humanities, and to a lesser extent in the social sciences. However, the real situation is far more complex and subject to a wide variety of disciplinary and institutional factors.

Amongst the complications are potential distortions in the system from recruitment and individual evaluation processes which, in some disciplines, impose on academics an obligation to produce books in order to get employment and promotion within academic environments. We know about publish or perish as a danger to quality outputs, but maybe we should also look more into the ‘not-publish-and-not-get-a-job’ syndrome because job seeking and promotion can be the only motive between behind publishing a book that very few people, apart from an evaluation committee will actually read.

To look at the effects of evaluation policy on researcher outputs in terms of monographs, I shall refer to three types of published works: doctoral dissertations, conference proceedings and collective works, and, to an extent, individual academic publications, but in all cases looking at motivations rather than the typologies of outputs.

Within ENRESSH, subgroup, NBABE – non-Bibliometric approaches to book evaluation – was set up to look more closely at qualitative and policy issues that can underlie or result from use, or misuse, of bibliometric data, and to complement the ENRESSH bibliometric studies and draw on their findings. In this paper, I also draw on data from the RobinBa study (Williams et al 2018) carried out in Italy in 2016. This is particularly useful as in addition to bibliometric analyses and questionnaires, we are also able to form focus groups to encourage SSH researcher interaction. What NBABE and RobinBa underlines is that we need to know where and why people are publishing, in other words, what their motivations actually are. This is vital if we want to get SSH researchers on board and to go beyond the current situation where they can be wary of evaluation, but still apply their own unwritten evaluation rules, without considering the consequences of the criteria they apply.

Researcher outlooks on books can be summed up well in three extracts from the RobinBa questionnaire:

• _« Printed paper is necessary to have my work recognized by employers/ those who might give me promotion; »_

• _« Because they are considered normal venues for scientific publications in evaluation processes »_

• _« books are the means to gather the results of a long term research; they should show not only new insights , but also present and discuss the sources »_

These clearly show the two main issues that are institutional recognition and evaluation protocols, and thus a potential dichotomy between indicator driven motivation, and genuine research motivation. These two can be complementary, or conflictual in terms of output quality, and leading to overproduction arising from a necessity to achieve institutional recognition rather than recognition from peers within a research community.

Amongst issues NBABE highlighted, there is the question as to whether there are not simply too many books and that consequently there is a drive to mediocrity, which is detrimental to quality. If we look at the situation in academia both between and just after WWII, we can see that many of the great names in academia published very little, and yet their fame was assured and the books that they published were extremely influential in many cases. From the 60s on, the development of mass higher education meant the mass recruitment of lecturing and research staff, and this has increasingly led to intense competition between candidates, who need to have more substantial publication profiles. Recruitment is a market and creating a market inevitably meant the rise of pay-for-publish companies alongside the traditional university publishing presses. Whilst the latter might have strong quality criteria, the former are commercial enterprises that need to generate profits.

Researchers have to play the system, so at this point, it must be said that one of the first tasks we must fulfil in book evaluation is defining what is a book. This may seem obvious, but as we have found numerous ways in which researchers fight the system, it is not so. A book has an ISBN, and if it has an ISBN, it is a book, is a circular argument: the amount of data, type of publication process and distribution of the product should be considered, in addition to questions of distribution - whether it is found online, in bookshops or libraries, and whether people actually read it. These are factors which are of great importance when discussing evaluation of books, but are issues dealt with by others on this panel.

As anyone who has reviewed recruitment profiles, particularly perhaps in the USA where tenure track positions are hard to find, will say, the pressure to publish a book is enormous, and starts even before entering academia.

For most young academics, their first document of book size is a thesis. The question as to whether the candidate has published a thesis, or not, is not necessarily an explicit indicator, but is often applied by recruitment panels in some humanities disciplines. Those imposing such a requirement fail to ask whether it is good to publish a thesis so soon after a viva and whether it might be better to make it available as is and wait until a more mature publication can be achieved. This is simply a perceived norm, and not one that has ever been thought through. The result is pressure, costs and hurry, often leading to a poor book from a good thesis. A thesis is not meant to be a book and even reworked, it often remains a thesis. Doctoral theses are important documents and we should ask whether it is appropriate that authors be forced into rapid publication, often at their cost, simply so as to be able to apply for a job, which they might not even get. It can also be argued that ideally a thesis should be made available in open access so that they are readable immediately, along with key source data, in the drive towards open science.

Promoting requires a change in evaluation protocols and an increase of awareness of the consequences of evaluative choices rather than imposing what can be an outdated or inappropriate mantra. It is easier to change formal evaluation protocols that have clear criteria than those of many recruitment panels where criteria have been handed down and where a true understanding of evaluation is lacking. The problem current is that whilst recruitment panels continue to impose, openly or not as many rules are unwritten disciplinary expectations, some less scrupulous publishers will continue to find academia find a very lucrative business. A move to open publishing of doctoral theses does not see open access as a lesser outlet, far from it, it is the first step to making people understand the value of open science.

Beyond the barriers to entering the system, we need to look at why are academics in some areas publish books. As in quotations from our database have shown, RobinBa holds some clues with one researcher being notably lucid when saying that:

« _Some major topics deserve books, other (less massive but relevant as well) need articles. If well connected with other chapters, book chapters too may be the best way to treat an object. Editing and supervising the production of a miscellany work may be a relevant scientific task, if the work is not conceive as a mere sum of self-referencial contributes._ »

Thus, a book should be a desire to publish research that is beyond the size of a single article or of the book chapter. The motivation is clearly dissemination of research which is perceive to be important. Perception is important as it is the researcher who should choose what is best in view of the content rather than following research external indicators. Thus, the controlling factors are the research community and the selection procedure of the publishers themselves, who need to apply some form of genuine selection procedure coupled with peer review, recognised series editors, and a clear distribution system. If this is the case for monographs, conference proceedings and collected works also require scrutiny.

Our interviewed researcher clearly differentiates between collected works and proceedings. The former should follow the same selection criteria as monographs, whilst proceedings raise other issues. Conference proceedings are an important area of publication, and as interviewees point out have the advantage of making knowledge and perspectives immediately available. A good conference is about bringing colleagues together, networking around a central theme, and disseminating knowledge.

It is not a question of judging whether a conference is good or bad, but it is a question of making the data available in a suitable format, and this within the perspective of open science. Here again, feedback from focus groups showed that confirmed researchers clearly differentiate between conference proceedings and a book chapter, which require a great deal more revision and inclusion in thematically valid publications. To quote « _conference proceedings are important because a conference on particular subjects is the way to better compare different methodologies_. »

It is the pressure to publish that means that proceedings can become disguised as collective works. Open access could give rapid access to data when precisely the purpose of a conference is to bring new knowledge immediately to a research community.

Given that motivations to publish may lead the use of pay-to-publish outlets purely to meet criteria that are not based so much on a need to disseminate knowledge, but a need to bolster a CV, we need to rethink policy. Changes in evaluation protocols should help, but quality controls and the requirement for genuine distribution networks could be key factors in judging outputs

In making science available, open access is essential, and making available primary publication data – doctoral dissertations and conference proceedings - is a first start. Obviously, open access cannot be seen as the only means of publication and should not become a constraint. Commercial publishing will remain important, particularly in hybrid publishing where a work of academic value can also have a wide public appeal, as it often the case with history. This means that evaluation systems must consider the importance of hybrid publications, but also that criteria are needed to judge what material can enter an academic book database, and that wider distributed works also achieve recognition.

When judging a printed work quality is a dangerous phenomenon. What is good to one is not to another and only discipline specialists, and even then, can judge validity of a publication. Maybe it is better to look at the publishers themselves to see what criteria they apply and whether they reach academic standards. Factors include looking at whether the publisher has valid peer review, whether there is an academically pertinent series editor, and whether the book will be actively distributed. Open access can take all these boxes and is obviously a way forward in many cases, provided we keep a close eye on exorbitant author publishing charges. The second issue is making sure that indicators are understood and not simply blindly applied, above all, policy statements are clear as to what the evaluation is about, and with no secret agenda. Book evaluation should not be about imposing models or disciplines, it is about looking at disciplines and rendering them more responsible, and thereby actually promoting the value of the social sciences and humanities. Book evaluation can never be seen separately from the global vision of what academia is about.

People play the system and we cannot blame people for trying to survive a system imposed by an increasingly technocratic system that is often overwilling to misuse bibliometric procedures. This is neither the fault of the researcher nor bibliometrics, which are of inestimable value if correctly used. It is the fault of badly thought out policy that fails to address researcher motivations and an alienation of researchers by a failure to consult them and imposing one-size fits all approaches from other fields with different histories and expectations.

To sum up, one thing is certain is that it doesn’t matter how good your indicators are if you do not have evaluators who understand evaluation and are not just ticking boxes. Training to evaluate is key as no one is a born evaluator and learning to stand back from one’s own prejudices is an acquired art. Evaluating requires research and thought as to the outcomes. It should be applied policy mixed to policy applied.